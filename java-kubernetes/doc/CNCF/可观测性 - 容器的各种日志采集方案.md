
在讨论各种日志采集方案之前，我们最好先聊一聊服务容器化后，日志的输出标准到底是该打印到标准输出还是落盘到文件。

Splunk 老牌日志收集方案，在每台容器宿主机上安装Splunk Agent即可。读日志文件，需要容器按一定规则把日志打到统一输出，Splunk收集到发送到远程服务端， 
优点:可以跨Kubenetes集群，并且跨容器和主机，作为一个全局统一的日志收集平台。 并且支持Splunk SQL解析日志，甚至可用来从日志信息分析接口成功率。自身带用户系统。  
缺点：需要提前主机上安装agent，需要约束容器的日志文件，需要管理员手工配置Splunk采集规则。

ELK  开源日志解决方案，需要每台容器宿主机上安装FileBeat。优点是免费，使用基本可同Splunk，少一个Splunk SQL，少一个配置统一下发功能。
规则配置在每台主机的FileBeat conf文件上，不便于统一管理，自己借助ansible实现规则配置可能是个方案。也可把FileBeat放到每个Pod里。
前端展示依赖Kinbana，需要控制用户权限就不方便做了。

Logging Operator 开源云原生日志解决方案。可以采主机、容器的日志。

Loki  开源云原生日志解决方案。可以采主机、容器的日志。  
只会对你的日志元数据标签（就像 Prometheus 的标签一样）进行索引，而不会对原始的日志数据进行全文索引。无法grep日志的任意字段。但实际日志是人打的，你并不是要查任意字段，而是你打出来的所有字段。






参考文档：  
https://github.com/banzaicloud/logging-operator
https://zhuanlan.zhihu.com/p/363427483

https://github.com/grafana/loki
https://www.qikqiak.com/post/grafana-loki-usage/